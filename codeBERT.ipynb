{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[OK] Loaded combined jsonl: 4003 samples\n",
      "[OK] Split combined -> train=3241 val=361 test=401\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, platform, torch, time\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ---------- 小网格搜索 ----------\n",
    "def cfg_key(cfg: dict) -> str:\n",
    "    return \"|\".join(f\"{k}={cfg[k]}\" for k in sorted(cfg))\n",
    "\n",
    "TUNE_DIR = Path(\"outputs/tuning\"); TUNE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_CSV = TUNE_DIR / \"search_results.csv\"\n",
    "\n",
    "# 搜索空间（先从少量高性价比超参开始；需要再扩可加）\n",
    "SEARCH_SPACE = {\n",
    "    \"LR\":            [1e-5],\n",
    "    \"WARMUP_RATIO\":  [0.06],\n",
    "    \"WEIGHT_DECAY\":  [0.01],\n",
    "    \"GRAD_ACCUM\":    [8],\n",
    "    \"DROPOUT_RATE\":  [0.1],\n",
    "    \"HEAD_WIDTH\":    [1.0],\n",
    "    \"HEAD_LR_MULT\":  [3.0],   # 新增\n",
    "    \"EPOCHS\":        [20],\n",
    "    \"MAX_GRAD_NORM\": [1.0],            # 想加裁剪就放开\n",
    "    \"NEG_WEIGHT\": [1, 1.5, 2.0],\n",
    "    \"POOL\": [\"topk2\"],\n",
    "}\n",
    "\n",
    "keys, values = zip(*SEARCH_SPACE.items())\n",
    "rows = []\n",
    "start_ts = int(time.time())\n",
    "\n",
    "HYPER_COLS = sorted(SEARCH_SPACE.keys())\n",
    "FIELDNAMES = (\n",
    "    [\"config\"] + HYPER_COLS +\n",
    "    [\"val_auc\", \"val_f1\", \"val_acc\", \"test_auc\", \"test_f1\", \"test_acc\", \"ckpt\", \"run_dir\"]\n",
    ")\n",
    "\n",
    "def pick_threshold(y_true, y_prob, target=\"f1\", min_precision=None):\n",
    "    P, R, T = precision_recall_curve(y_true, y_prob)\n",
    "    # sklearn 返回的 T 长度比 P/R 少 1；下面索引会相应对齐\n",
    "    if min_precision is not None:\n",
    "        mask = P[:-1] >= min_precision\n",
    "        if mask.any():\n",
    "            idx = mask.argmax()\n",
    "            return T[idx]\n",
    "    if target == \"f1\":\n",
    "        f1 = 2*P*R/(P+R+1e-12)\n",
    "        idx = np.nanargmax(f1[:-1])\n",
    "        return T[idx]\n",
    "    return 0.5\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, mode=\"max\", min_delta=1e-4):\n",
    "        self.best = None\n",
    "        self.bad = 0\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "    def step(self, value):\n",
    "        if self.best is None:\n",
    "            self.best = value\n",
    "            return False\n",
    "        improved = (value > self.best + self.min_delta) if self.mode==\"max\" else (value < self.best - self.min_delta)\n",
    "        if improved:\n",
    "            self.best = value\n",
    "            self.bad = 0\n",
    "        else:\n",
    "            self.bad += 1\n",
    "        return self.bad > self.patience\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    precision_recall_curve,\n",
    "    f1_score, accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "\n",
    "# ---- Config ----\n",
    "COMBINED_JSONL = Path(r\"D:\\learning\\APS360\\project\\leetcode_github\\combined_train\\all.jsonl\")\n",
    "MODEL_NAME  = \"microsoft/codebert-base\"   # or \"Salesforce/codet5-base\"\n",
    "OUT_DIR     = Path(\"outputs/code_model_nb\")\n",
    "MAX_LENGTH  = 512\n",
    "CHUNK_STRIDE = 256  # 新增\n",
    "BATCH_SIZE  = 8\n",
    "EVAL_BATCH  = 8\n",
    "EPOCHS      = 20\n",
    "LR          = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "GRAD_ACCUM   = 1\n",
    "NUM_WORKERS  = 2\n",
    "SEED         = 42\n",
    "USE_FP16     = True                       # mixed precision on CUDA if available\n",
    "FORCE_CPU    = False                      # set True to force CPU\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not FORCE_CPU else \"cpu\")\n",
    "print(f\"[INFO] Device: {device}\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            rows.append(json.loads(ln))\n",
    "    return rows\n",
    "\n",
    "# 从 combined_train/all.jsonl 读取所有样本\n",
    "all_rows = read_jsonl(COMBINED_JSONL)\n",
    "print(f\"[OK] Loaded combined jsonl: {len(all_rows)} samples\")\n",
    "\n",
    "# 按 label 做一次 stratified train/val/test 划分\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_labels = [int(r.get(\"label\", 0)) for r in all_rows]\n",
    "\n",
    "# 先拆出 test 10%\n",
    "train_val_rows, test_rows, train_val_y, test_y = train_test_split(\n",
    "    all_rows, all_labels, test_size=0.10, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "# 再从 train_val 里拆出 val 10%\n",
    "train_rows, val_rows, train_y, val_y = train_test_split(\n",
    "    train_val_rows, train_val_y, test_size=0.10, random_state=42, stratify=train_val_y\n",
    ")\n",
    "\n",
    "print(f\"[OK] Split combined -> train={len(train_rows)} val={len(val_rows)} test={len(test_rows)}\")\n",
    "\n",
    "class CodeJsonlDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item returns:\n",
    "      {\n",
    "        \"chunks\": List[ Dict[input_ids, attention_mask] ],\n",
    "        \"label\": float (0.0/1.0)\n",
    "      }\n",
    "    We first encode without special tokens, then slice token ids into\n",
    "    blocks of (max_length - 2), and call `prepare_for_model` per block.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "        for row in data:\n",
    "            text = row.get(\"text\", \"\")\n",
    "            label = float(row.get(\"label\", 0))\n",
    "            chunks = self._tokenize_to_chunks(text)\n",
    "            self.samples.append({\"chunks\": chunks, \"label\": label})\n",
    "\n",
    "    def _tokenize_to_chunks(self, text: str):\n",
    "        ids = self.tokenizer.encode(text, add_special_tokens=False, truncation=False)\n",
    "        if len(ids) == 0:\n",
    "            enc = self.tokenizer(\"\", truncation=True, max_length=self.max_length, return_attention_mask=True)\n",
    "            return [enc]\n",
    "\n",
    "        block = self.max_length - 2\n",
    "        chunks = []\n",
    "        step = max(1, (block - CHUNK_STRIDE))\n",
    "        for start in range(0, len(ids), step):\n",
    "            seg = ids[start:start+block]\n",
    "            enc = self.tokenizer.prepare_for_model(\n",
    "                seg, truncation=True, max_length=self.max_length,\n",
    "                add_special_tokens=True, return_attention_mask=True\n",
    "            )\n",
    "            chunks.append({\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]})\n",
    "        return chunks\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "\n",
    "class ChunkCollator:\n",
    "    \"\"\"\n",
    "    Collate a batch of variable-length chunk lists.\n",
    "    Returns:\n",
    "      input_ids       : (num_chunks, L)\n",
    "      attention_mask  : (num_chunks, L)\n",
    "      group_bounds    : 1D tensor of length (B+1), cumulative chunk counts per sample\n",
    "      labels          : (B,)\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, pad_to_multiple_of: int = 8):\n",
    "        self.tok = tokenizer\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        ids_list, att_list = [], []\n",
    "        group_bounds = [0]\n",
    "        labels = []\n",
    "\n",
    "        for ex in batch:\n",
    "            labels.append(ex[\"label\"])\n",
    "            for ch in ex[\"chunks\"]:\n",
    "                ids_list.append(torch.tensor(ch[\"input_ids\"], dtype=torch.long))\n",
    "                att_list.append(torch.tensor(ch[\"attention_mask\"], dtype=torch.long))\n",
    "            group_bounds.append(group_bounds[-1] + len(ex[\"chunks\"]))\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            ids_list, batch_first=True, padding_value=self.tok.pad_token_id\n",
    "        )\n",
    "        attention = torch.nn.utils.rnn.pad_sequence(\n",
    "            att_list, batch_first=True, padding_value=0\n",
    "        )\n",
    "\n",
    "        if self.pad_to_multiple_of:\n",
    "            L = input_ids.size(1)\n",
    "            pad_len = (self.pad_to_multiple_of - L % self.pad_to_multiple_of) % self.pad_to_multiple_of\n",
    "            if pad_len > 0:\n",
    "                pad_ids = torch.full((input_ids.size(0), pad_len), self.tok.pad_token_id, dtype=torch.long)\n",
    "                pad_att = torch.zeros((attention.size(0), pad_len), dtype=torch.long)\n",
    "                input_ids = torch.cat([input_ids, pad_ids], dim=1)\n",
    "                attention = torch.cat([attention, pad_att], dim=1)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        group_bounds = torch.tensor(group_bounds, dtype=torch.long)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention,\n",
    "                \"group_bounds\": group_bounds, \"labels\": labels}\n",
    "\n",
    "class CodeClassifier(nn.Module):\n",
    "    def __init__(self, model_name, dropout_rate=0.1, hidden_mult=2, pos_weight=None, pool=\"logit_mean\"):\n",
    "        super().__init__()\n",
    "        self.config  = AutoConfig.from_pretrained(model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden      = self.encoder.config.hidden_size\n",
    "        mid         = int(hidden * hidden_mult)\n",
    "        self.pool   = pool\n",
    "        self.topk_k = 2\n",
    "        self.att_vec = nn.Linear(hidden, 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden, mid),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mid, 1)\n",
    "        )\n",
    "\n",
    "        if pos_weight is not None:\n",
    "            pw = torch.as_tensor(pos_weight).reshape(1)\n",
    "            self.register_buffer(\"pos_weight\", pw)   # 作为 buffer 跟随 device\n",
    "            self.use_pos_weight = True\n",
    "        else:\n",
    "            # 仍然定义属性，避免 forward 里找不到\n",
    "            self.register_buffer(\"pos_weight\", torch.tensor([1.0]), persistent=False)  # 占位，不使用\n",
    "            self.use_pos_weight = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, group_bounds, labels=None):\n",
    "        # 1) 编码得到每个 token 的隐状态\n",
    "        out  = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last = out.last_hidden_state                         # (num_chunks, L, H)\n",
    "\n",
    "        # 2) 对每个 chunk 做 masked mean，得到 chunk 级表征\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(last)    # (num_chunks, L, 1)\n",
    "        chunk_repr = (last * mask).sum(1) / mask.sum(1).clamp(min=1)  # (num_chunks, H)\n",
    "\n",
    "        # 3) 逐 chunk 过分类头，得到 chunk 级 logit\n",
    "        chunk_logits = self.classifier(chunk_repr).squeeze(-1)        # (num_chunks,)\n",
    "\n",
    "        # 4) 聚合 chunk → sample\n",
    "        B = group_bounds.numel() - 1\n",
    "        sample_logits = []\n",
    "        for i in range(B):\n",
    "            s, e = int(group_bounds[i]), int(group_bounds[i + 1])\n",
    "            if e > s:\n",
    "                if self.pool == \"topk2\":\n",
    "                    # --- Top-k 概率平均（更鲁棒） ---\n",
    "                    p = torch.sigmoid(chunk_logits[s:e])\n",
    "                    k = getattr(self, \"topk_k\", 2)\n",
    "                    k = min(k, e - s)\n",
    "                    if k > 0:\n",
    "                        topk = p.topk(k).values.mean()\n",
    "                        topk = topk.clamp(1e-6, 1 - 1e-6)\n",
    "                        logit = torch.log(topk / (1 - topk))\n",
    "                    else:\n",
    "                        logit = chunk_logits.new_zeros(())\n",
    "                    sample_logits.append(logit)\n",
    "                else:\n",
    "                    # --- 原版 mean 聚合 ---\n",
    "                    sample_logits.append(chunk_logits[s:e].mean())\n",
    "            else:\n",
    "                sample_logits.append(chunk_logits.new_zeros(()))\n",
    "\n",
    "        logits = torch.stack(sample_logits, dim=0)                     # (B,)\n",
    "\n",
    "        # 5) 组织输出；只有提供 labels 时才计算 loss\n",
    "        result = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            neg_w = cfg.get(\"NEG_WEIGHT\", 1.5)\n",
    "            w = torch.ones_like(labels, dtype=logits.dtype, device=logits.device)\n",
    "            w[labels == 0] = neg_w\n",
    "            result[\"loss\"] = F.binary_cross_entropy_with_logits(\n",
    "                logits, labels.float(), weight=w\n",
    "            )\n",
    "        return result\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token or tokenizer.sep_token\n",
    "\n",
    "train_ds = CodeJsonlDataset(train_rows, tokenizer, MAX_LENGTH)\n",
    "val_ds   = CodeJsonlDataset(val_rows, tokenizer, MAX_LENGTH)\n",
    "test_ds  = CodeJsonlDataset(test_rows, tokenizer, MAX_LENGTH)\n",
    "\n",
    "collator = ChunkCollator(tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "'''train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collator)\n",
    "val_loader   = DataLoader(val_ds, batch_size=EVAL_BATCH, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collator)\n",
    "test_loader  = DataLoader(test_ds, batch_size=EVAL_BATCH, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collator)'''\n",
    "PIN = (device.type == \"cuda\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0, pin_memory=PIN, collate_fn=collator, timeout=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=EVAL_BATCH, shuffle=False,\n",
    "    num_workers=0, pin_memory=PIN, collate_fn=collator, timeout=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=EVAL_BATCH, shuffle=False,\n",
    "    num_workers=0, pin_memory=PIN, collate_fn=collator, timeout=0\n",
    ")\n",
    "\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n",
    "\n",
    "# == 统计训练集正负样本，计算 pos_weight ==\n",
    "labels = [float(r[\"label\"]) for r in train_rows]\n",
    "N_pos = sum(labels)\n",
    "N_neg = len(labels) - N_pos\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# 避免频繁分配带来的碎片化（可选）\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Train / Evaluate (with EarlyStopping + Grid Search)\n",
    "\n",
    "\n",
    "import os, math, time, csv, itertools, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from safetensors.torch import save_file, load_file\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_y, all_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            ids   = batch[\"input_ids\"].to(device)\n",
    "            attn  = batch[\"attention_mask\"].to(device)\n",
    "            gb    = batch[\"group_bounds\"].to(device)\n",
    "            y     = batch[\"labels\"].to(device)\n",
    "\n",
    "            out = model(ids, attn, gb, labels=None)\n",
    "            prob = torch.sigmoid(out[\"logits\"]).cpu().numpy()\n",
    "            all_prob.append(prob)\n",
    "            all_y.append(y.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_y)\n",
    "    y_prob = np.concatenate(all_prob)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    try:\n",
    "        auc_val = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc_val = float(\"nan\")\n",
    "\n",
    "    metrics = {\n",
    "        \"auc\": float(auc_val),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"cm\": confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ---------- 单次训练：按 cfg 训练到早停 ----------\n",
    "def train_eval_once(cfg: dict, run_dir: Path, seed: int = 42):\n",
    "    \"\"\"\n",
    "    cfg: {\n",
    "      'LR': float,\n",
    "      'WARMUP_RATIO': float,\n",
    "      'WEIGHT_DECAY': float,\n",
    "      'GRAD_ACCUM': int,\n",
    "      'EPOCHS': int\n",
    "    }\n",
    "    复用全局的: MODEL_NAME, device, USE_FP16, train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Build model / optimizer / scheduler / scaler\n",
    "    model = CodeClassifier(\n",
    "        MODEL_NAME,\n",
    "        dropout_rate=cfg[\"DROPOUT_RATE\"],\n",
    "        hidden_mult=cfg[\"HEAD_WIDTH\"],\n",
    "        pos_weight=None,\n",
    "        pool=cfg.get(\"POOL\", \"logit_mean\"),\n",
    "        neg_weight=cfg.get(\"NEG_WEIGHT\", 1.5),\n",
    "    ).to(device)\n",
    "\n",
    "    # ↓↓↓ 加这两行：显著降显存占用（牺牲一点速度）\n",
    "    try:\n",
    "        model.encoder.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for i, layer in enumerate(model.encoder.encoder.layer):\n",
    "        if i < 1:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "    torch.cuda.empty_cache()  # 清理上一次 config 的遗留碎片\n",
    "    head_params = list(model.classifier.parameters())\n",
    "    backbone_params = [p for n,p in model.named_parameters() if not n.startswith(\"classifier\")]\n",
    "    optimizer = AdamW(\n",
    "        [\n",
    "            {\"params\": backbone_params, \"lr\": cfg[\"LR\"]},\n",
    "            {\"params\": head_params,     \"lr\": cfg[\"LR\"] * cfg.get(\"HEAD_LR_MULT\", 1.0)},\n",
    "        ],\n",
    "        weight_decay=cfg[\"WEIGHT_DECAY\"]\n",
    "    )\n",
    "\n",
    "    total_steps  = cfg[\"EPOCHS\"] * math.ceil(len(train_loader) / cfg[\"GRAD_ACCUM\"])\n",
    "    warmup_steps = min(200, int(total_steps * cfg[\"WARMUP_RATIO\"]))\n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(USE_FP16 and device.type == \"cuda\"))\n",
    "    early  = EarlyStopper(patience=2, mode=\"max\")\n",
    "\n",
    "    best_auc   = -1.0\n",
    "    best_path  = run_dir / \"best.safetensors\"\n",
    "    global_step = 0\n",
    "    # 2) Train loop with early stop\n",
    "    for epoch in range(1, cfg[\"EPOCHS\"] + 1):\n",
    "        running_loss = 0.0\n",
    "        num_steps = 0\n",
    "        model.train()\n",
    "        prog = tqdm(train_loader, desc=f\"Train epoch {epoch}\",\n",
    "                    leave=False, dynamic_ncols=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        first_optim_step_done = False\n",
    "        accum = int(cfg.get(\"GRAD_ACCUM\", 1))\n",
    "        max_gn = float(cfg.get(\"MAX_GRAD_NORM\", 0.0))\n",
    "\n",
    "        # ------------ 训练步循环【注意：在 epoch 里面】------------\n",
    "        for step, batch in enumerate(prog, start=1):\n",
    "            try:\n",
    "                ids  = batch[\"input_ids\"].to(device)\n",
    "                attn = batch[\"attention_mask\"].to(device)\n",
    "                gb   = batch[\"group_bounds\"].to(device)\n",
    "                y    = batch[\"labels\"].to(device)\n",
    "\n",
    "                with torch.amp.autocast(\"cuda\", enabled=(USE_FP16 and device.type == \"cuda\")):\n",
    "                    out = model(ids, attn, gb, labels=y)\n",
    "                    loss_raw = out[\"loss\"]\n",
    "                    loss = loss_raw / accum\n",
    "\n",
    "                running_loss += float(loss_raw.detach().item())\n",
    "                num_steps += 1\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if step % accum == 0:\n",
    "                    if max_gn > 0:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_gn)\n",
    "\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    if first_optim_step_done:\n",
    "                        scheduler.step()\n",
    "                    else:\n",
    "                        first_optim_step_done = True\n",
    "\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                # 进度条信息\n",
    "                try:\n",
    "                    current_lr = scheduler.get_last_lr()[0] if first_optim_step_done else cfg[\"LR\"]\n",
    "                except Exception:\n",
    "                    current_lr = cfg[\"LR\"]\n",
    "\n",
    "                avg_loss = running_loss / max(1, num_steps)\n",
    "                prog.set_postfix(step=step, avg_loss=f\"{avg_loss:.4f}\", lr=f\"{current_lr:.1e}\")\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        # （可选）尾包刷新：若未被 accum 整除，按需补一次 step\n",
    "        # ...\n",
    "\n",
    "        prog.close()  # 关闭本轮进度条，避免后面的 print 被插断\n",
    "\n",
    "        # ------------ 验证与日志【注意：也在 epoch 里面】------------\n",
    "        avg_train_loss = running_loss / max(1, num_steps)\n",
    "        print(f\"[TRAIN] epoch={epoch} avg_loss={avg_train_loss:.4f}\")\n",
    "\n",
    "        val_m = evaluate(model, val_loader, device)\n",
    "        print(f\"[VAL] AUC={val_m['auc']:.4f}  F1={val_m['f1']:.4f}  \"\n",
    "            f\"ACC={val_m['acc']:.4f}  P={val_m['precision']:.4f}  R={val_m['recall']:.4f}\")\n",
    "\n",
    "        if (not np.isnan(val_m[\"auc\"])) and (val_m[\"auc\"] > best_auc):\n",
    "            best_auc = val_m[\"auc\"]\n",
    "            save_file(model.state_dict(), str(best_path))\n",
    "\n",
    "            # 选阈值（例如按 F1 最大）\n",
    "            val_thr = pick_threshold(val_m[\"y_true\"], val_m[\"y_prob\"], min_precision=0.65)\n",
    "            print(f\"[VAL] Picked threshold = {val_thr:.3f}\")\n",
    "            with (run_dir / \"best_threshold.txt\").open(\"w\") as f:\n",
    "                f.write(str(val_thr))\n",
    "\n",
    "            with (run_dir / \"val_metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"auc\": val_m[\"auc\"], \"f1\": float(val_m[\"f1\"]),\n",
    "                    \"acc\": float(val_m[\"acc\"]), \"precision\": float(val_m[\"precision\"]),\n",
    "                    \"recall\": float(val_m[\"recall\"]), \"threshold\": float(val_thr)\n",
    "                }, f, indent=2)\n",
    "            print(f\"[OK] Saved best (safetensors) -> {best_path}\")\n",
    "\n",
    "\n",
    "        if early.step(val_m[\"auc\"]):\n",
    "            print(f\"[EARLY STOP] Best AUC={early.best:.4f}\")\n",
    "            break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    state_dict = load_file(str(best_path))\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    val_m  = evaluate(model, val_loader, device)\n",
    "    val_thr = pick_threshold(val_m[\"y_true\"], val_m[\"y_prob\"], min_precision=0.75)\n",
    "    (run_dir / \"best_threshold.txt\").write_text(str(val_thr))\n",
    "    print(f\"[VAL] Recomputed threshold on best = {val_thr:.3f}\")\n",
    "\n",
    "    # 3) Evaluate TEST on best\n",
    "    state_dict = load_file(str(best_path))\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    test_m = evaluate(model, test_loader, device)  # 先拿到 y_true / y_prob\n",
    "\n",
    "    # 读取验证时保存的最佳阈值\n",
    "    try:\n",
    "        val_thr = float((run_dir / \"best_threshold.txt\").read_text().strip())\n",
    "    except Exception:\n",
    "        val_thr = 0.5\n",
    "\n",
    "    y_true = test_m[\"y_true\"]\n",
    "    y_prob = test_m[\"y_prob\"]\n",
    "    y_pred = (y_prob >= val_thr).astype(int)\n",
    "\n",
    "    # 诊断：分数分布是否在 val/test 发生漂移\n",
    "    print(\n",
    "        f\"[SCORE SHIFT] val mean prob={float(val_m['y_prob'].mean()):.3f}  \"\n",
    "        f\"test mean prob={float(test_m['y_prob'].mean()):.3f}\"\n",
    "    )\n",
    "\n",
    "    test_at_thr = {\n",
    "        \"auc\":  float(roc_auc_score(y_true, y_prob)),\n",
    "        \"f1\":   float(f1_score(y_true, y_pred)),\n",
    "        \"acc\":  float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\":    float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"cm\": confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    with (run_dir / \"test_metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"auc\": test_at_thr[\"auc\"], \"f1\": test_at_thr[\"f1\"],\n",
    "            \"acc\": test_at_thr[\"acc\"], \"precision\": test_at_thr[\"precision\"],\n",
    "            \"recall\": test_at_thr[\"recall\"], \"cm\": test_at_thr[\"cm\"].tolist(),\n",
    "            \"threshold\": float(val_thr),\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(\"[TEST]\", json.dumps({\n",
    "        \"auc\": test_at_thr[\"auc\"], \"f1\": test_at_thr[\"f1\"],\n",
    "        \"acc\": test_at_thr[\"acc\"], \"precision\": test_at_thr[\"precision\"],\n",
    "        \"recall\": test_at_thr[\"recall\"]\n",
    "    }, indent=2))\n",
    "\n",
    "    return {\"val\": val_m, \"test\": test_at_thr, \"ckpt\": str(best_path)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da42d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss=0.6461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4839, F1=0.000\n",
      "Epoch 2: train loss=0.6306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4986, F1=0.000\n",
      "Epoch 3: train loss=0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4932, F1=0.000\n",
      "Epoch 4: train loss=0.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4928, F1=0.138\n",
      "Epoch 5: train loss=0.5846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4929, F1=0.209\n",
      "Epoch 6: train loss=0.5685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4926, F1=0.306\n",
      "Epoch 7: train loss=0.5597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4926, F1=0.132\n",
      "Epoch 8: train loss=0.5130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4881, F1=0.414\n",
      "Epoch 9: train loss=0.4578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4896, F1=0.296\n",
      "Epoch 10: train loss=0.3885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4826, F1=0.409\n",
      "Epoch 11: train loss=0.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4854, F1=0.105\n",
      "Epoch 12: train loss=0.2509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4973, F1=0.157\n",
      "Epoch 13: train loss=0.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.5024, F1=0.491\n",
      "Epoch 14: train loss=0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4979, F1=0.131\n",
      "Epoch 15: train loss=0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → val AUC=0.4993, F1=0.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# === 过拟合能力测试 ===\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "N_SMALL = 64   # 选 64 个样本\n",
    "subset_idx = random.sample(range(len(train_ds)), N_SMALL)\n",
    "small_train_ds = Subset(train_ds, subset_idx)\n",
    "\n",
    "small_train_loader = DataLoader(\n",
    "    small_train_ds, batch_size=8, shuffle=True,\n",
    "    num_workers=0, pin_memory=(device.type==\"cuda\"),\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "# 用相同 val_loader\n",
    "model = CodeClassifier(\n",
    "    MODEL_NAME,\n",
    "    dropout_rate=cfg[\"DROPOUT_RATE\"],\n",
    "    hidden_mult=cfg[\"HEAD_WIDTH\"],\n",
    "    pos_weight=None,\n",
    "    pool=cfg.get(\"POOL\", \"logit_mean\"),\n",
    ").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for batch in small_train_loader:\n",
    "        ids, attn, gb, y = (batch[\"input_ids\"].to(device),\n",
    "                            batch[\"attention_mask\"].to(device),\n",
    "                            batch[\"group_bounds\"].to(device),\n",
    "                            batch[\"labels\"].to(device))\n",
    "        out = model(ids, attn, gb, labels=y)\n",
    "        loss = out[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: train loss={total/len(small_train_loader):.4f}\")\n",
    "    val_m = evaluate(model, val_loader, device)\n",
    "    print(f\" → val AUC={val_m['auc']:.4f}, F1={val_m['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc944ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learning\\APS360\\project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n",
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=1 avg_loss=0.8502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.7768  F1=0.0000  ACC=0.5485  P=0.0000  R=0.0000\n",
      "[VAL] Picked threshold = 0.379\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_199660170\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=2 avg_loss=0.7490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.8866  F1=0.8011  ACC=0.8061  P=0.7460  R=0.8650\n",
      "[VAL] Picked threshold = 0.376\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_199660170\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=3 avg_loss=0.4696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9491  F1=0.8605  ACC=0.8670  P=0.8177  R=0.9080\n",
      "[VAL] Picked threshold = 0.098\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_199660170\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=4 avg_loss=0.3497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9617  F1=0.8495  ACC=0.8753  P=0.9338  R=0.7791\n",
      "[VAL] Picked threshold = 0.016\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_199660170\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=5 avg_loss=0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9693  F1=0.8851  ACC=0.8892  P=0.8324  R=0.9448\n",
      "[VAL] Picked threshold = 0.027\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_199660170\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=6 avg_loss=0.2763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9713  F1=0.8896  ACC=0.8975  P=0.8663  R=0.9141\n",
      "[VAL] Picked threshold = 0.023\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_199660170\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=7 avg_loss=0.2617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9711  F1=0.8812  ACC=0.8864  P=0.8352  R=0.9325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=8 avg_loss=0.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9671  F1=0.8786  ACC=0.8837  P=0.8306  R=0.9325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=9 avg_loss=0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9658  F1=0.8739  ACC=0.8809  P=0.8371  R=0.9141\n",
      "[EARLY STOP] Best AUC=0.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Recomputed threshold on best = 0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCORE SHIFT] val mean prob=0.466  test mean prob=0.476\n",
      "[TEST] {\n",
      "  \"auc\": 0.9744098442993471,\n",
      "  \"f1\": 0.7735042735042735,\n",
      "  \"acc\": 0.7356608478802993,\n",
      "  \"precision\": 0.6306620209059234,\n",
      "  \"recall\": 1.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=1 avg_loss=0.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.5510  F1=0.0000  ACC=0.5485  P=0.0000  R=0.0000\n",
      "[VAL] Picked threshold = 0.306\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_175185057\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=2 avg_loss=0.8221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.8034  F1=0.0000  ACC=0.5485  P=0.0000  R=0.0000\n",
      "[VAL] Picked threshold = 0.337\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_175185057\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=3 avg_loss=0.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.8584  F1=0.6494  ACC=0.7368  P=0.8148  R=0.5399\n",
      "[VAL] Picked threshold = 0.166\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_175185057\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=4 avg_loss=0.5080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9383  F1=0.8468  ACC=0.8587  P=0.8294  R=0.8650\n",
      "[VAL] Picked threshold = 0.162\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_175185057\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=5 avg_loss=0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9558  F1=0.8650  ACC=0.8781  P=0.8650  R=0.8650\n",
      "[VAL] Picked threshold = 0.048\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_175185057\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=6 avg_loss=0.3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9663  F1=0.8647  ACC=0.8864  P=0.9357  R=0.8037\n",
      "[VAL] Picked threshold = 0.021\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_175185057\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=7 avg_loss=0.3038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9643  F1=0.8682  ACC=0.8864  P=0.9122  R=0.8282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=8 avg_loss=0.2858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9557  F1=0.8459  ACC=0.8587  P=0.8333  R=0.8589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=9 avg_loss=0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9673  F1=0.8544  ACC=0.8753  P=0.9041  R=0.8098\n",
      "[VAL] Picked threshold = 0.007\n",
      "[OK] Saved best (safetensors) -> outputs\\tuning\\run_1764395687_175185057\\best.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=10 avg_loss=0.2466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9658  F1=0.8696  ACC=0.8837  P=0.8805  R=0.8589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=11 avg_loss=0.2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9660  F1=0.8854  ACC=0.8975  P=0.8938  R=0.8773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] epoch=12 avg_loss=0.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] AUC=0.9660  F1=0.8815  ACC=0.8920  P=0.8735  R=0.8896\n",
      "[EARLY STOP] Best AUC=0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Recomputed threshold on best = 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCORE SHIFT] val mean prob=0.403  test mean prob=0.409\n",
      "[TEST] {\n",
      "  \"auc\": 0.967754897036665,\n",
      "  \"f1\": 0.7718550106609808,\n",
      "  \"acc\": 0.7331670822942643,\n",
      "  \"precision\": 0.6284722222222222,\n",
      "  \"recall\": 1.0\n",
      "}\n",
      "\n",
      "=================== Best by val_auc ===================\n",
      "config: DROPOUT_RATE=0.1|EPOCHS=20|GRAD_ACCUM=8|HEAD_LR_MULT=3.0|HEAD_WIDTH=1.0|LR=1e-05|MAX_GRAD_NORM=1.0|NEG_WEIGHT=1.2|POOL=topk2|WARMUP_RATIO=0.06|WEIGHT_DECAY=0.01\n",
      "LR: 1e-05\n",
      "WARMUP_RATIO: 0.06\n",
      "WEIGHT_DECAY: 0.01\n",
      "GRAD_ACCUM: 8\n",
      "EPOCHS: 20\n",
      "val_auc: 0.9713391584557228\n",
      "val_f1: 0.8895522388059701\n",
      "test_auc: 0.9744098442993471\n",
      "test_f1: 0.7735042735042735\n",
      "ckpt: outputs\\tuning\\run_1764395687_199660170\\best.safetensors\n",
      "run_dir: outputs\\tuning\\run_1764395687_199660170\n"
     ]
    }
   ],
   "source": [
    "with open(RESULTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "    writer = csv.DictWriter(fcsv, fieldnames=FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for combo in itertools.product(*values):\n",
    "        cfg = dict(zip(keys, combo))\n",
    "        run_name = f\"run_{start_ts}_{hash('|'.join(f'{k}={cfg[k]}' for k in sorted(cfg))) & 0xfffffff}\"\n",
    "        run_dir  = TUNE_DIR / run_name\n",
    "\n",
    "        try:\n",
    "            result = train_eval_once(cfg, run_dir, seed=42)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"[OOM] Skipping config due to CUDA OOM:\", \"|\".join(f\"{k}={cfg[k]}\" for k in sorted(cfg)))\n",
    "            torch.cuda.empty_cache()\n",
    "            # 只写我们声明过的列\n",
    "            row = {\n",
    "                \"config\": \"|\".join(f\"{k}={cfg[k]}\" for k in sorted(cfg)),\n",
    "                **{k: cfg[k] for k in HYPER_COLS},\n",
    "                \"val_auc\": 0.0, \"val_f1\": 0.0, \"val_acc\": 0.0,\n",
    "                \"test_auc\": 0.0, \"test_f1\": 0.0, \"test_acc\": 0.0,\n",
    "                \"ckpt\": \"OOM\", \"run_dir\": str(run_dir)\n",
    "            }\n",
    "            rows.append(row)\n",
    "            writer.writerow(row); fcsv.flush()\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"config\": \"|\".join(f\"{k}={cfg[k]}\" for k in sorted(cfg)),\n",
    "            **{k: cfg[k] for k in HYPER_COLS},\n",
    "            \"val_auc\":  float(result[\"val\"][\"auc\"]),\n",
    "            \"val_f1\":   float(result[\"val\"][\"f1\"]),\n",
    "            \"val_acc\":  float(result[\"val\"][\"acc\"]),\n",
    "            \"test_auc\": float(result[\"test\"][\"auc\"]),\n",
    "            \"test_f1\":  float(result[\"test\"][\"f1\"]),\n",
    "            \"test_acc\": float(result[\"test\"][\"acc\"]),\n",
    "            \"ckpt\":     result[\"ckpt\"],\n",
    "            \"run_dir\":  str(run_dir)\n",
    "        }\n",
    "        rows.append(row)\n",
    "        writer.writerow(row); fcsv.flush()\n",
    "\n",
    "# 选最优\n",
    "rows_sorted = sorted(rows, key=lambda r: r[\"val_auc\"], reverse=True)\n",
    "best = rows_sorted[0]\n",
    "print(\"\\n=================== Best by val_auc ===================\")\n",
    "for k in [\"config\",\"LR\",\"WARMUP_RATIO\",\"WEIGHT_DECAY\",\"GRAD_ACCUM\",\"EPOCHS\",\"val_auc\",\"val_f1\",\"test_auc\",\"test_f1\",\"ckpt\",\"run_dir\"]:\n",
    "    print(f\"{k}: {best[k]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (aig-venv)",
   "language": "python",
   "name": "aig-venv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
